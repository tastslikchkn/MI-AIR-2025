<p align="center">
  <img src="logo_IA.jpg" alt="Conference Logo" width="1000"/>
</p>

# MI/AIR 2025 Conference Abstracts üéâ

<p align="center">
  <a href="https://sites.google.com/miair.org/miair2023/home">
    <img src="https://lh6.googleusercontent.com/VuND54NjC7Z3bU7bZUDFnIU4Fqi033z4YpPcYUcLeZI2tlWxG4DnAGt9IrNr9RVOOHM6rSFi1dWy_eBhDWDNPVGSbTV2Gcre_D2rV7ODgIGAfph73xsahhA2hnPhn0CW4IZK2zq9JX3OIhfYXOtuBt38nz-7O4Qd9q46OXvTnhuONFK8RqLDmg=w1280" alt="MI/AIR Conference Logo" width="300"/>
  </a>
</p>

[Visit the MI/AIR 2025 Conference Website](https://sites.google.com/miair.org/miair2023/home)

## Abstract 1: DIY Local LLM Bootcamp: From Zero to Inference in 45 Minutes ü§ñ
**Authors:**  
**Affiliation:**  

**Abstract:**  
This presentation introduces foundational concepts of open-source large language models (LLMs) and the benefits of hosting them locally. Attendees will learn how to run the most cutting-edge open-source models on premises. Running open source LLMs locally enhances privacy, reduces cloud costs, and provides flexibility for customization. We‚Äôll explore tools such as Ollama and LM Studio, which simplify local LLM deployment by serving RESTful API endpoints to bridge the gap between a model and application. The session will cover installing and configuring Ollama and LM Studio, loading popular LLMs, and basic inference configurations. Participants will also learn how to monitor local resources and how to choose appropriate models and adjust parameters of the model to stay within their hardware‚Äôs capabilities. Lastly, attendees will be given examples of interacting with the models via command-line interfaces and built in Retrieval-Augmented Generation (RAG) chat bot. More in depth topics will be covered in Part2 and 3.

**Keywords:** Open-source LLMs, Ollama, LM Studio, Local Deployment, API Basics üõ†Ô∏è

---

## Abstract 2: Bridging Local LLMs to Apps: Local API‚Äôs and GitHub Repos üîó
**Authors:**  
**Affiliation:**  

**Abstract:**  
Building on the Part 1 session‚Äôs foundational knowledge, this presentation dives into integrating local large language models (LLM) into open-source applications. We‚Äôll explore leveraging RESTful API endpoints to connect LLMs with tools like chatbots, data analysis platforms, custom scripts, and Integrated Development Environments (IDE). Topics include performance optimization for low resource environments, training analytics platforms. Hands-on demos will showcase how to integrate LLMs into workflows using Python‚Äôs requests library and toolkits like LangChain. The session concludes with a live demonstration of IDE integration that enables real-time agentic coding powered by local models. This demo sets the stage for Part 3, where we‚Äôll explore Vibe Coding in depth. If you plan to attend it is suggested that you register for Part 1 and 3.

**Keywords:** LLM APIs, Model Integration, Security Best Practices, Performance Optimization üöÄ

---

## Abstract 3: Vibe Coding with Local LLMs: Create your own coding agent in VSCode üíª
**Authors:**  
**Affiliation:**  

**Abstract:**  
The final presentation introduces Vibe Coding, a paradigm where local LLMs directly enhance coding productivity through real-time suggestions. We‚Äôll explore how analysts with a basic understanding of coding and logic can leverage their locally hosted models to auto-complete code, debug errors, and generate documentation in IDEs such as VSCode. The session will cover configuring VSCode for specific coding tasks (e.g., Python, HTML, CSS, JavaScript), customizing model behavior with prompts, and enhancing capabilities via leading edge extensions. We will develop software using hierarchical prompting techniques. Attendees will be equipped to implement LLM-powered workflows tailored to their development needs while maintaining full control over data privacy and computational resources. If you plan to attend it is suggested that you register for Part 1 and 2.

**Keywords:** Vibe Coding, PRDs, Task Lists, IDE Integration, Custom Prompts ‚ú®
